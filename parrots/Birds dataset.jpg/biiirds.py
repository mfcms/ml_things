import tensorflow as tf
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout
from tensorflow.keras.models import Model
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix
import clip
import torch
from sklearn.svm import SVC

from tensorflow.keras.applications import MobileNetV2
base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))

IMG_SIZE = 224

# –ó–∞–≥—Ä—É–∂–∞–µ–º –±–µ–∑ –≥–æ–ª–æ–≤—ã, –≤–µ—Å–∞ ImageNet
base_model = EfficientNetB0(
    weights='imagenet',
    include_top=False,
    input_shape=(IMG_SIZE, IMG_SIZE, 3)
)
base_model.trainable = False  # üîí –ó–ê–ú–û–†–û–ó–ò–õ–ò

# –î–æ–±–∞–≤–ª—è–µ–º –ª—ë–≥–∫—É—é –≥–æ–ª–æ–≤—É
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)  # ‚Üë dropout ‚Äî –ø—Ä–æ—Ç–∏–≤ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
x = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)
predictions = Dense(4, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=predictions)
model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-3),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

base_model = EfficientNetB0(
    weights='imagenet',
    include_top=False,
    input_shape=(224, 224, 3)
)

# –ó–∞–º–æ—Ä–æ–∑–∏–º —á–∞—Å—Ç—å —Å–ª–æ—ë–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ ‚Äî –º–æ–∂–Ω–æ –∑–∞–º–æ—Ä–æ–∑–∏—Ç—å –ø–µ—Ä–≤—ã–µ ~100 —Å–ª–æ—ë–≤)
base_model.trainable = True  # –∏–ª–∏ False ‚Üí –ø–æ—Ç–æ–º fine-tune

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dropout(0.3)(x)
predictions = Dense(4, activation='softmax')(x)  # 4 –∫–ª–∞—Å—Å–∞

model = Model(inputs=base_model.input, outputs=predictions)

model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-4),  # –Ω–∏–∑–∫–∏–π LR –¥–ª—è fine-tuning
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=30,           # ¬±30¬∞
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.15,
    zoom_range=0.2,
    horizontal_flip=True,
    brightness_range=[0.7, 1.3], # –º–µ–Ω—è–µ–º —è—Ä–∫–æ—Å—Ç—å ‚Äî –ø–æ–º–æ–≥–∞–µ—Ç –ø—Ä–∏ —Ä–∞–∑–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö –æ—Å–≤–µ—â–µ–Ω–∏—è
    fill_mode='nearest'
)

val_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    'dataset/train',
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=8,  # –º–∞–ª–µ–Ω—å–∫–∏–π batch ‚Äî —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ –ø—Ä–∏ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    class_mode='categorical'
)
val_generator = val_datagen.flow_from_directory(
    'dataset/val',
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=8,
    class_mode='categorical'
)
callbacks = [
    tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),
    tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3),
    tf.keras.callbacks.ModelCheckpoint('best_parrot_model.h5', save_best_only=True)
]

history = model.fit(
    train_generator,
    epochs=30,
    validation_data=val_generator,
    callbacks=callbacks
)

model, preprocess = clip.load("ViT-B/32")